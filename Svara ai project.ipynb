{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"F:\\\\Programming\\\\Artificial intelligence projects\\\\Svana AI\\\\reply_classification_dataset.csv\")\n",
    "\"\"\"\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df['label'].value_counts())\n",
    "\"\"\"\n",
    "# Clean label column\n",
    "df['label'] = df['label'].str.lower().str.strip()  # lowercase + remove spaces\n",
    "\n",
    "# Remove punctuation like commas\n",
    "df['label'] = df['label'].str.replace(r'[^a-z]', '', regex=True)\n",
    "\n",
    "print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598fa5ee",
   "metadata": {},
   "source": [
    "# Reading CSV and cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b23a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "DATA_PATH = \"F:\\\\Programming\\\\Artificial intelligence projects\\\\Svana AI\\\\reply_classification_dataset.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Quick look\n",
    "#print(df.head())\n",
    "#print(df.info())\n",
    "\n",
    "# Check label distribution\n",
    "#print(df['label'].value_counts())\n",
    "\n",
    "# cleaing data using\n",
    "\n",
    "# Normalize labels\n",
    "df['label'] = df['label'].str.lower().str.strip().str.replace(r'[^a-z]', '', regex=True)\n",
    "\n",
    "print(df['label'].value_counts())\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)   # keep only letters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['reply'].apply(clean_text)\n",
    "#print(df[['reply', 'clean_text']].head())\n",
    "\n",
    "#Train/Test Split\n",
    "#We need to separate training and testing data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['clean_text']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640032c",
   "metadata": {},
   "source": [
    "# Training the baseline model using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d26da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Convert text to vectors\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = log_reg.predict(X_test_vec)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df555b3",
   "metadata": {},
   "source": [
    "# Fine tunig with Hugging face model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd135620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import re\n",
    "\n",
    "DATA_PATH = \"F:\\\\Programming\\\\Artificial intelligence projects\\\\Svana AI\\\\reply_classification_dataset.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "\n",
    "# Normalize labels (make them lowercase, remove junk)\n",
    "df['label'] = df['label'].str.lower().str.strip().str.replace(r'[^a-z]', '', regex=True)\n",
    "\n",
    "# Clean the text (lowercase, remove special chars)\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Keep letters and spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['reply'].apply(clean_text)\n",
    "\n",
    "# Remove duplicates\n",
    "df_unique = df.drop_duplicates(subset=['clean_text', 'label'])\n",
    "\n",
    "# Map labels to numbers (transformers need numbers: neutral=0, negative=1, positive=2)\n",
    "label_map = {'neutral': 0, 'negative': 1, 'positive': 2}\n",
    "df_unique['label_id'] = df_unique['label'].map(label_map)\n",
    "\n",
    "# Split into train and test (80% train, 20% test)\n",
    "train_df, test_df = train_test_split(df_unique, test_size=0.2, random_state=42, stratify=df_unique['label'])\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df[['clean_text', 'label_id']].rename(columns={'clean_text': 'text', 'label_id': 'label'}))\n",
    "test_dataset = Dataset.from_pandas(test_df[['clean_text', 'label_id']].rename(columns={'clean_text': 'text', 'label_id': 'label'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b3f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "num_labels = 3  # neutral, negative, positive\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f05c9d",
   "metadata": {},
   "source": [
    "# Comparison between Fine tuned model and Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e91954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Load dataset\n",
    "DATA_PATH = \"F:\\\\Programming\\\\Artificial intelligence projects\\\\Svana AI\\\\reply_classification_dataset.csv\"\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Normalize labels\n",
    "data['label'] = data['label'].str.lower().str.strip().str.replace(r'[^a-z]', '', regex=True)\n",
    "\n",
    "# Check label distribution\n",
    "print(data['label'].value_counts())\n",
    "\n",
    "# Map labels to numerical values\n",
    "label_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "data['label'] = data['label'].map(label_map)\n",
    "\n",
    "# Handle missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Keep only letters and spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "data['clean_text'] = data['reply'].apply(clean_text)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['clean_text'], data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Baseline Model: Logistic Regression with TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "lr_preds = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate baseline\n",
    "lr_accuracy = accuracy_score(y_test, lr_preds)\n",
    "lr_f1 = f1_score(y_test, lr_preds, average='weighted')\n",
    "print(f\"Logistic Regression - Accuracy: {lr_accuracy:.4f}, F1 Score: {lr_f1:.4f}\")\n",
    "\n",
    "# Transformer Model: DistilBERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_data(texts):\n",
    "    return tokenizer(texts.tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "train_encodings = tokenize_data(X_train)\n",
    "test_encodings = tokenize_data(X_test)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "class EmailDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = EmailDataset(train_encodings, y_train.tolist())\n",
    "test_dataset = EmailDataset(test_encodings, y_test.tolist())\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    eval_strategy='epoch'  # Changed from evaluation_strategy to eval_strategy\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "transformer_preds = trainer.predict(test_dataset).predictions.argmax(-1)\n",
    "\n",
    "# Evaluate transformer\n",
    "transformer_accuracy = accuracy_score(y_test, transformer_preds)\n",
    "transformer_f1 = f1_score(y_test, transformer_preds, average='weighted')\n",
    "print(f\"DistilBERT - Accuracy: {transformer_accuracy:.4f}, F1 Score: {transformer_f1:.4f}\")\n",
    "\n",
    "# Model comparison\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(f\"Logistic Regression - Accuracy: {lr_accuracy:.4f}, F1 Score: {lr_f1:.4f}\")\n",
    "print(f\"DistilBERT - Accuracy: {transformer_accuracy:.4f}, F1 Score: {transformer_f1:.4f}\")\n",
    "#print(\"Recommendation: Choose DistilBERT for production due to its ability to capture contextual nuances in short replies, despite higher computational cost. Logistic Regression is faster for prototyping but may miss subtle sentiment patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb706e6",
   "metadata": {},
   "source": [
    "# Test Run of comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecca109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Sentiment Classification Script\n",
    "# ===============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Load & preprocess dataset\n",
    "# -------------------------------\n",
    "DATA_PATH = \"F:\\\\Programming\\\\Artificial intelligence projects\\\\Svana AI\\\\reply_classification_dataset.csv\"\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Normalize labels\n",
    "data['label'] = data['label'].str.lower().str.strip().str.replace(r'[^a-z]', '', regex=True)\n",
    "\n",
    "# Map labels -> integers\n",
    "label_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "data['label'] = data['label'].map(label_map)\n",
    "\n",
    "# Drop rows with missing or unmapped labels\n",
    "data = data.dropna(subset=['label', 'reply'])\n",
    "\n",
    "# Keep two versions of text: raw (for BERT), cleaned (for TF-IDF baseline)\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)        # keep only letters and spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # normalize spaces\n",
    "    return text\n",
    "\n",
    "data['raw_text'] = data['reply'].astype(str).str.strip()\n",
    "data['clean_text'] = data['reply'].apply(clean_text)\n",
    "\n",
    "# -------------------------------\n",
    "# Train-test split (stratified)\n",
    "# -------------------------------\n",
    "X_raw = data['raw_text']\n",
    "X_clean = data['clean_text']\n",
    "y = data['label']\n",
    "\n",
    "X_train_raw, X_test_raw, X_train_clean, X_test_clean, y_train, y_test = train_test_split(\n",
    "    X_raw, X_clean, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Baseline: Logistic Regression\n",
    "# -------------------------------\n",
    "print(\"\\n===== Baseline: Logistic Regression (TF-IDF) =====\")\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2), min_df=3)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_clean)\n",
    "X_test_tfidf = vectorizer.transform(X_test_clean)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "lr_preds = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "print(classification_report(y_test, lr_preds, target_names=['positive','neutral','negative']))\n",
    "\n",
    "# -------------------------------\n",
    "# Transformer: DistilBERT\n",
    "# -------------------------------\n",
    "print(\"\\n===== Transformer: DistilBERT Fine-tuning =====\")\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(texts.tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "train_encodings = tokenize_texts(X_train_raw)\n",
    "test_encodings = tokenize_texts(X_test_raw)\n",
    "\n",
    "class EmailDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k,v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(int(self.labels[idx]))\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = EmailDataset(train_encodings, y_train)\n",
    "test_dataset = EmailDataset(test_encodings, y_test)\n",
    "\n",
    "# Weighted loss (optional if class imbalance is large)\n",
    "counts = Counter(y_train)\n",
    "total = sum(counts.values())\n",
    "weights = [total / counts[i] for i in range(3)]\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k:v for k,v in inputs.items() if k!=\"labels\"})\n",
    "        logits = outputs.logits\n",
    "        loss_fct = CrossEntropyLoss(weight=class_weights.to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Metrics function\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    num_train_epochs=3,        # try 3-4 first\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate DistilBERT\n",
    "preds = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(preds.predictions, axis=1)\n",
    "print(classification_report(y_test, pred_labels, target_names=['positive','neutral','negative']))\n",
    "\n",
    "# -------------------------------\n",
    "# Save fine-tuned model\n",
    "# -------------------------------\n",
    "finetuned_path = \"F:\\\\Programming\\\\Artificial intelligence projects\\\\Svana AI\\\\Project\\\\Models\\\\distilbert-finetuned\"\n",
    "model.save_pretrained(finetuned_path)\n",
    "tokenizer.save_pretrained(finetuned_path)\n",
    "\n",
    "print(\"\\nTraining complete. Models saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d48b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lr(texts, vectorizer, model):\n",
    "    # preprocess same as training\n",
    "    clean_texts = [re.sub(r'[^a-z\\s]', '', t.lower()) for t in texts]\n",
    "    clean_texts = [re.sub(r'\\s+', ' ', t).strip() for t in clean_texts]\n",
    "    X_tfidf = vectorizer.transform(clean_texts)\n",
    "    preds = model.predict(X_tfidf)\n",
    "    label_map_inv = {0:'positive', 1:'neutral', 2:'negative'}\n",
    "    return [label_map_inv[p] for p in preds]\n",
    "\n",
    "# Example test\n",
    "test_samples = [\n",
    "    \"I really love this product!\",\n",
    "    \"It's okay, nothing special.\",\n",
    "    \"This is terrible, I hate it.\"\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Logistic Regression Predictions:\")\n",
    "print(predict_lr(test_samples, vectorizer, lr_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a31994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bert(texts, tokenizer, model):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "    preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "    label_map_inv = {0:'positive', 1:'neutral', 2:'negative'}\n",
    "    return [label_map_inv[p] for p in preds]\n",
    "\n",
    "print(\"DistilBERT Predictions:\")\n",
    "print(predict_bert(test_samples, tokenizer, model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ec060",
   "metadata": {},
   "source": [
    "# Fast API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e236d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Load Logistic Regression model and TF-IDF vectorizer\n",
    "MODEL_PATH = \"f:\\\\Programming\\\\Artificial intelligence projects\\\\Svana AI\\\\Project\\\\Models\\\\lr_model.joblib\"\n",
    "VECTORIZER_PATH = \"f:\\\\Programming\\\\Artificial intelligence projects\\\\Svana AI\\\\Project\\\\Models\\\\tfidf_vectorizer.joblib\"\n",
    "lr_model = joblib.load(MODEL_PATH)\n",
    "vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "\n",
    "label_map = {0: \"positive\", 1: \"neutral\", 2: \"negative\"}\n",
    "\n",
    "def clean_text(text):\n",
    "\ttext = text.lower()\n",
    "\ttext = re.sub(r'[^a-z\\s]', '', text)\n",
    "\ttext = re.sub(r'\\s+', ' ', text).strip()\n",
    "\treturn text\n",
    "\n",
    "class TextRequest(BaseModel):\n",
    "\ttext: str\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict_sentiment(request: TextRequest):\n",
    "\tcleaned = clean_text(request.text)\n",
    "\tX = vectorizer.transform([cleaned])\n",
    "\tpred = lr_model.predict(X)[0]\n",
    "\tlabel = label_map.get(pred, \"unknown\")\n",
    "\treturn {\"label\": label, \"label_id\": int(pred)}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
